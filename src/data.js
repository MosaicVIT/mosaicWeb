export const articledata = {
	_id: {
		$oid: "67f26fcdd58c29fb2f61fe1b",
	},
	id: "fa12e3d9722ba8f15bf097affd4be24d",
	source: {
		platform: "arxiv.org",
		url: "https://arxiv.org/abs/2407.14962",
		scrape_timestamp: "2025-04-06T15:02:49.984479",
	},
	metadata: {
		title: "Recent Advances in Generative AI and Large Language Models",
		authors: ["Desta Haileselassie Hagos", "Rick Battle", "Danda B. Rawat"],
		published_date: "2024-08-23T14:14:21Z",
		created_at: "2025-04-06T09:33:02.396769",
		updated_at: "2025-04-06T09:33:02.396769",
	},
	content: {
		description:
			"Computer Science > Computation and Language\narXiv:2407.14962 (cs)\n[Submitted on 20 Jul 2024 (v1), last revised 23 Aug 2024 (this version, v5)]\nTitle:Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives\nAuthors:Desta Haileselassie Hagos, Rick Battle, Danda B. Rawat [...] [v1] Sat, 20 Jul 2024 18:48:35 UTC (2,640 KB)\n[v2] Tue, 23 Jul 2024 18:07:28 UTC (2,639 KB)\n[v3] Fri, 2 Aug 2024 14:26:55 UTC (2,639 KB)\n[v4] Tue, 13 Aug 2024 19:17:32 UTC (2,641 KB)\n[v5] Fri, 23 Aug 2024 14:14:21 UTC (2,641 KB)  \nFull-text links:\nAccess Paper:\nView a PDF of the paper titled Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives, by Desta Haileselassie Hagos and 2 other authors",
		abstract:
			"This comprehensive article explores recent advancements in generative AI and large language models as detailed in the paper by Hagos, Battle, and Rawat. It discusses the current state of technology, the challenges related to scalability, biases, and computational demands, and offers perspectives on future research directions including model interpretability and ethical deployment. The discussion provides both historical context and technical insights into the transformative impact of these technologies, making the study a crucial reference for researchers and practitioners alike.",
		article_text:
			'Introduction\n\nIn recent years, generative AI and large language models (LLMs) have emerged as pivotal drivers in the evolution of computational linguistics and artificial intelligence. The paper titled "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives" has significantly contributed to this evolving field by outlining the milestones, current status, and future directions of generative AI research. The study, led by authors Desta Haileselassie Hagos, Rick Battle, and Danda B. Rawat, presents an in-depth analysis of the technological advancements in language understanding and generation.\n\nBody\n\nThe article provides a comprehensive overview of the current state of generative AI, emphasizing the scaling of language models and the innovative application of deep learning techniques for natural language processing. It details how recent advances have led to the development of sophisticated models that can effectively generate human-like text, addressing challenges such as contextual understanding, semantic nuance, and real-world applicability. An important discussion in the paper revolves around the balance between model size and efficiency, considering that larger models often lead to improved performance but also introduce complexities such as increased computational costs and the potential for biased outputs.\n\nFurthermore, the paper explores the integral challenges in this rapidly developing area. These issues include the need for robust evaluation metrics, ethical considerations, and the expansion of these models to better serve diverse languages and contexts. The authors also outline future perspectives, suggesting that ongoing research should focus on enhancing model interpretability and reliability while reducing resource-intensive operations. The discussion is enriched by a historical context and technical explanations that demystify the underlying mechanisms making modern generative AI a transformative tool in various industries.\n\nConclusion\n\nOverall, the paper not only documents the state-of-the-art advancements in generative AI and LLMs but also serves as a beacon for future research directions. It encourages the need for continuous evolution and careful consideration of ethical practices in the deployment of these powerful technologies. As AI continues to redefine the boundaries of what is possible, the insights provided by this research remain vital in guiding the industry toward more sustainable and inclusive innovations.',
		keywords: [
			"Generative AI",
			"Large Language Models",
			"Computational Linguistics",
			"Deep Learning",
			"Artificial Intelligence",
			"Ethical AI",
			"Future Perspectives",
		],
	},
	raw_text:
		"Computer Science > Computation and Language\narXiv:2407.14962 (cs)\n[Submitted on 20 Jul 2024 (v1), last revised 23 Aug 2024 (this version, v5)]\nTitle:Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives\nAuthors:Desta Haileselassie Hagos, Rick Battle, Danda B. Rawat [...] [v1] Sat, 20 Jul 2024 18:48:35 UTC (2,640 KB)\n[v2] Tue, 23 Jul 2024 18:07:28 UTC (2,639 KB)\n[v3] Fri, 2 Aug 2024 14:26:55 UTC (2,639 KB)\n[v4] Tue, 13 Aug 2024 19:17:32 UTC (2,641 KB)\n[v5] Fri, 23 Aug 2024 14:14:21 UTC (2,641 KB)  \nFull-text links:\nAccess Paper:\nView a PDF of the paper titled Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives, by Desta Haileselassie Hagos and 2 other authors",
	scraper_metadata: {
		agent_version: "1.0.0",
		extraction_confidence: 0.8,
		errors: [],
	},
	type: "Article",
	assigned_tags: [
		"Generative AI",
		"Research Paper",
		"ArXiv",
		"Advanced",
		"Transformer",
	],
	tags_assigned_at: {
		$date: "2025-04-06T12:16:21.318Z",
	},
};

export const modeldata = {
	_id: "67f258c0d58c29fb2f61ba14",
	Model: "GPT-4.5",
	Domain: "Language,Vision,Multimodal",
	Task: "Language modeling/generation,Question answering,Quantitative reasoning,Translation,Visual question answering,Code generation,Instruction interpretation",
	Authors:
		"Foundational contributors\nAlex Paino, Ali Kamali, Amin Tootoonchian, Andrew Tulloch, Ben Sokolowsky, Clemens Winter, Colin Wei, Daniel Kappler, Daniel Levy, Felipe Petroski Such, Geoff Salmon, Ian O’Connell, Jason Teplitz, Kai Chen, Nik Tezak, Prafulla Dhariwal, Rapha Gontijo Lopes, Sam Schoenholz, Youlong Cheng, Yujia Jin, Yunxing Dai\n\nResearch\nCore contributors\n\nAiden Low, Alec Radford, Alex Carney, Alex Nichol, Alexis Conneau, Ananya Kumar, Ben Wang, Charlotte Cole , Elizabeth Yang, Gabriel Goh, Hadi Salman, Haitang Hu, Heewoo Jun, Ian Sohl, Ishaan Gulrajani, Jacob Coxon, James Betker, Jamie Kiros, Jessica Landon, Kyle Luther, Lia Guy, Lukas Kondraciuk, Lyric Doshi, Mikhail Pavlov, Qiming Yuan, Reimar Leike, Rowan Zellers, Sean Metzger, Shengjia Zhao, Spencer Papay, Tao Wang\n\nContributors\n\nAdam Lerer, Aidan McLaughlin, Alexander Prokofiev, Alexandra Barr, Allan Jabri, Ananya Kumar, Andrew Gibiansky, Andrew Schmidt, Casey Chu, Chak Li, Chelsea Voss, Chris Hallacy, Chris Koch, Christine McLeavey, David Mely, Dimitris Tsipras, Eric Sigler, Erin Kavanaugh, Farzad Khorasani, Huiwen Chang, Ilya Kostrikov, Ishaan Singal, Ji Lin, Jiahui Yu, Jing Yu Zhang, John Rizzo, Jong Wook Kim, Joyce Lee, Juntang Zhuang, Leo Liu, Li Jing, Long Ouyang, Louis Feuvrier, Mo Bavarian, Nick Stathas, Nitish Keskar, Oleg Murk, Preston Bowman, Scottie Yan, SQ Mah, Tao Xu, Taylor Gordon, Valerie Qi, Wenda Zhou, Yu Zhang\n\nScaling\nCore contributors\n\nAdam Goucher, Alex Chow, Alex Renzin, Aleksandra Spyra, Avi Nayak, Ben Leimberger, Christopher Hesse, Duc Phong Nguyen, Dinghua Li, Eric Peterson, Francis Zhang, Gene Oden, Kai Fricke, Kai Hayashi, Larry Lv, Leqi Zou, Lin Yang, Madeleine Thompson, Michael Petrov, Miguel Castro, Natalia Gimelshein, Phil Tillet, Reza Zamani, Ryan Cheu Stanley Hsieh, Steve Lee, Stewart Hall, Thomas Raoux, Tianhao Zheng, Vishal Kuo, Yongjik Kim, Yuchen Zhang, Zhuoran Liu\n\nContributors\n\nAlvin Wan, Andrew Cann, Antoine Pelisse, Anuj Kalia, Aaron Hurst, Avital Oliver, Brad Barnes, Brian Hsu, Chen Ding, Chen Shen, Cheng Chang, Christian Gibson, Duncan Findlay, Fan Wang, Fangyuan Li, Gianluca Borello, Heather Schmidt, Henrique Ponde de Oliveira Pinto, Ikai Lan, Jiayi Weng, James Crooks, Jos Kraaijeveld, Junru Shao, Kenny Hsu, Kenny Nguyen, Kevin King, Leah Burkhardt, Leo Chen, Linden Li, Lu Zhang, Mahmoud Eariby, Marat Dukhan, Mateusz Litwin, Miki Habryn, Natan LaFontaine, Pavel Belov, Peng Su, Prasad Chakka, Rachel Lim, Rajkumar Samuel, Renaud Gaubert, Rory Carmichael, Sarah Dong, Shantanu Jain, Stephen Logsdon, Todd Underwood, Weixing Zhang, Will Sheu, Weiyi Zheng, Yinghai Lu, Yunqiao Zhang\n\nSafety Systems\nAndrea Vallone, Andy Applebaum, Cameron Raymond, Chong Zhang, Dan Mossing, Elizabeth Proehl, Eric Wallace, Evan Mays, Grace Zhao, Ian Kivlichan, Irina Kofman, Joel Parish, Kevin Liu, Keren Gu-Lemberg, Kristen Ying, Lama Ahmad, Lilian Weng , Leon Maksin, Leyton Ho, Meghan Shah, Michael Lampe, Michele Wang, Miles Wang, Olivia Watkins, Phillip Guo, Samuel Miserendino, Sam Toizer, Sandhini Agarwal, Tejal Patwardhan, Tom Dupré la Tour, Tong Mu, Tyna Eloundou, Yunyun Wang\n\nDeployment\nAdam Brandon, Adam Perelman, Adele Li, Akshay Nathan, Alan Hayes, Alfred Xue, Alison Ben, Alec Gorge, Alex Guziel, Alex Iftimie, Ally Bennett, Andrew Chen, Andy Wang, Andy Wood, Angad Singh, Anoop Kotha, Antonia Woodford, Anuj Saharan, Ashley Tyra, Atty Eleti, Ben Schneider, Bessie Ji, Beth Hoover, Bill Chen, Blake Samic, Britney Smith, Brian Yu, Caleb Wang, Cary Bassin, Cary Hudson, Charlie Jatt, Chengdu Huang, Chris Beaumont, Christina Huang, Cristina Scheau, Dana Palmie, Daniel Levine, Daryl Neubieser, Dave Cummings, David Sasaki, Dibya Bhattacharjee, Dylan Hunn, Edwin Arbus, Elaine Ya Le, Enis Sert, Eric Kramer, Fred von Lohmann, Gaby Janatpour, Garrett McGrath, Garrett Ollinger, Gary Yang, Hao Sheng, Harold Hotelling, Janardhanan Vembunarayanan, Jeff Harris, Jeffrey Sabin Matsumoto, Jennifer Robinson, Jessica Liang, Jessica Shieh, Jiacheng Yang, Joel Morris, Joseph Florencio, Josh Kaplan, Kan Wu, Karan Sharma, Karen Li, Katie Pypes, Kendal Simon, Kendra Rimbach, Kevin Park, Kevin Rao, Laurance Fauconnet, Lauren Workman, Leher Pathak, Liang Wu, Liang Xiong, Lien Mamitsuka, Lindsay McCallum, Lukas Gross, Manoli Liodakis, Matt Nichols, Michelle Fradin, Minal Khan, Mingxuan Wang, Nacho Soto, Natalie Staudacher, Nikunj Handa, Niko Felix, Ning Liu, Olivier Godement, Oona Gleeson, Philip Pronin, Raymond Li, Reah Miyara, Rohan Nuttall, R.J. Marsan, Sara Culver, Scott Ethersmith, Sean Fitzgerald, Shamez Hemani, Sherwin Wu, Shiao Lee, Shuyang Cheng, Siyuan Fu, Spug Golden, Steve Coffey, Steven Heidel, Sundeep Tirumalareddy, Tabarak Khan, Thomas Degry, Thomas Dimson, Tom Stasi, Tomo Hiratsuka, Trevor Creech, Uzair Navid Iftikhar, Victoria Chernova, Victoria Spiegel, Wanning Jiang, Wenlei Xie, Yaming Lin, Yara Khakbaz, Yilei Qian, Yilong Qin, Yo Shavit, Zhi Bie\n\nExecutive Leadership\nBob McGrew, Greg Brockman, Hannah Wong, Jakub Pachocki, Johannes Heidecke, Joanne Jang, Kate Rouch, Kevin Weil, Lauren Itow, Liam Fedus, Mark Chen, Mia Glaese, Mira Murati, Nick Ryder, Sam Altman, Srinivas Narayanan, Tal Broda",
	"Model accessibility": "API access",
	Link: "https://openai.com/index/introducing-gpt-4-5/",
	Citations: "",
	Reference: "Introducing GPT-4.5",
	"Publication date": "2025-02-27",
	Organization: "OpenAI",
	Parameters: "",
	"Parameters notes": "",
	"Training compute (FLOP)": "",
	"Training compute notes": "",
	"Training dataset": "Unspecified unreleased",
	"Training dataset notes":
		'"GPT-4.5 was pre-trained and post-trained on diverse datasets, including a mix of publicly available\ndata, proprietary data from data partnerships, and custom datasets developed in-house, which\ncollectively contribute to the model’s robust conversational capabilities and world knowledge."',
	"Training dataset size (datapoints)": "",
	"Dataset size notes": "",
	"Training time (hours)": "",
	"Training time notes": "",
	"Training hardware": "",
	Confidence: "Unknown",
	Abstract:
		"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.\n\nUnsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.\nScaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.\nGPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",
	"Country (from Organization)": "United States of America",
	"Base model": "",
	"Finetune compute (FLOP)": "",
	"Finetune compute notes": "",
	"Hardware quantity": "",
	"Training code accessibility": "Unreleased",
	"Accessibility notes": "",
	"Organization categorization (from Organization)": "Industry",
	type: "Model",
};

export const productData = {
	_id: "67f258c1d58c29fb2f61fdb3",
	source: {
		platform: "Xploria",
		url: "https://www.aixploria.com/en/buni-ai/",
		scrape_timestamp: "2025-04-06T07:40:41.403753Z",
	},
	metadata: {
		title: "Buni AI",
		authors: [],
		published_date: "",
		repository_stats: {
			stars: 0,
			forks: 0,
			contributors: 0,
			last_updated: "",
		},
	},
	content: {
		description:
			"« Create high-quality content in minutes with text, image and code generation tools »",
		readme: "",
		abstract: "",
		code_snippets: [],
		images: [
			{
				url: "https://www.aixploria.com/wp-content/uploads/favicons/buni-ai.png",
				alt_text: "Tool logo",
				source_location: "scraped card",
			},
			{
				url: "https://www.aixploria.com/wp-content/uploads/2023/07/buni_ai.webp",
				alt_text: "Screenshot from #secondSpecialButton",
				source_location: "#secondSpecialButton",
			},
		],
	},
	raw_html: "",
	scraper_metadata: {
		agent_version: "1.0.0",
		extraction_confidence: 0.95,
		errors: [],
	},
	type: "AI Tool",
};